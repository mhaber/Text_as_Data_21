---
title: "Text as Data: Week 10"
author: "Matthias Haber"
date: "17 November 2021"
output:
    revealjs::revealjs_presentation:
      theme: moon
      reveal_plugins: ["notes", "zoom", "chalkboard"]
      highlight: haddock
      center: false
      self_contained: false
      incremental: true
      reveal_options:
        slideNumber: true
        progress: true
        previewLinks: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
library(tidyverse)
library(tidytext)
library(readtext)
library(quanteda)
theme_set(theme_minimal())
```

# Goals for Today

## Goals

- Organizational stuff
- Topic models

# Topic Models

## Topic models: basic idea

We often have collections of documents that we’d like to divide into natural groups so that we can understand them separately. Topic modeling is a method for unsupervised classification of such documents, which finds natural groups of items even when we’re not sure what we’re looking for.

## Topic models: basic idea

- Topic models are exploratory probability models that

    + weaken the contraints required in dictionary based content analysis
    + have been intensively studied in the computer science literature 

- Topic models work best with large amounts of text with a thematic structure

## Topic models: LDA

Latent Dirichlet allocation (LDA) is a popular method for fitting a topic model. It treats each document as a mixture of topics, and each topic as a mixture of words.LDA is a  method for estimating both of these at the same time: the bag of words associated with each topic, and the bag of topics that describe each document.

```{r, out.width = "600px", echo = F}
knitr::include_graphics("img/lda1.png")
```  

## Topic models: LDA (II)
We assume that some number of topics exists for the whole collection of documents. Each document is generated by first choosing a distribution over the topics, then, for each word, choosing a topic assignment and choosing the word from the corresponding topic

```{r, out.width = "600px", echo = F}
knitr::include_graphics("img/lda2.png")
```  

## Topic models: LDA (III)

```{r, out.width = "700px", echo = F}
knitr::include_graphics("img/lda3.png")
```  

## Topic model: LDA (IV)

- Topic models giveth:

    + a probabilistic view of the relationship between W, Z and $\theta$
    + a full statistical framework for learning most aspects of the relationship

- and taketh away:

    + substantive control: You do not get to assert what the topics mean (inevitable when the Z and $\theta$ are both unobserved)

## Topic model: Gibbs Sampling

- Topic models need to estimate lots of unknowns simultaneously and thus can be quite time consuming to estimate. To estimate the correct weights LDA uses Gibbs sampling, an algorithm for successively sampling conditional distributions of variables.

```{r, out.width = "600px", echo = F}
knitr::include_graphics("img/gibbs.jpeg")
``` 

## Application: policy agenda
- Quinn et al. analyze 118,065 congressional speeches from
1997-2004.

```{r, out.width = "500px", echo = F}
knitr::include_graphics("img/policyagenda.png")
```  

## Outbut $\beta$

```{r, out.width = "700px", echo = F}
knitr::include_graphics("img/definingTopics.png")
```  

## Output $\theta$

```{r, out.width = "600px", echo = F}
knitr::include_graphics("img/quinn_theta.png")
``` 

## Topic Model evaluation

- There are two main modes of evaluation:
    
    + Statistical
    + Human
  
- and two natural levels

    + The model as a whole: model fit, K, and topic relationships
    + Topic structure: word precision, topic coherence
    
## Construct validity

Procedure: 
1. Choose number of topics K
2. Fit Model
3. Label Topics
4. Cluster the $\beta^k$

```{r, out.width = "500px", echo = F}
knitr::include_graphics("img/clustering.png")
``` 

## Choosing K
The number of topics assumed a priori has a large effect on the results.

```{r, out.width = "600px", echo = F}
knitr::include_graphics("img/choice_topics.png")
``` 


## Variations: Seeded LDA

- Seeded LDA is a semi-supervised automated content analysis model and a variant of the standard LDA approach. While standard LDA does not assume the topics to be found a priori, seeded LDA uses “seed words” to weigh the prior distribution of topics  before fitting the model.

- R: ``install_packages("seededlda")`` (also comes with great diagnostic functions)


## Variations: Structural topic model

Structural topic models (STM) are similar to LDA models but allow to include metadata (the information about each document) into the topicmodel. 

```{r, out.width = "400px", echo = F}
knitr::include_graphics("img/stm.png")
``` 

- R: ``install_packages("stm")`` (also comes with great diagnostic functions)
 

## Variations: expressed agenda model

In a simpler variation on LDA, Grimmer (2009) defines an expressed agenda model as

```{r, out.width = "700px", echo = F}
knitr::include_graphics("img/grimmer.png")
``` 

- Here there are not multiple topics per press release, but there are
observed authors drawn from a population

- R: ``install_github("christophergandrud/ExpAgenda")``

## Variations: correlated topic models

- The Dirichlet multinomial assumptions hide a constraint about topic
covariation

    + LDA cannot represent free covariation of topic proportions
    + The correlated topic model can

- Replace the Dirichlet with a Logistic Normal structure (Aitchison, 1986)
with arbitrary covariance matrix

- R: ``topicmodels``

# Group Excercise 

## Topic model exercise: Load data

We will take another look at the US Senate debate on partial birth abortion.

```{r}
load("data/corpus_us_debate_speaker.rda")
summary(corpus_us_debate_speaker, n = 5)
```

## Topic model exercise: Reshape to paragraphs

The 23 speeches are probably too big to cover only one topic. So we'll reshape them to paragraphs treating each paragraph as a separate document instead. We can use the `corpus_reshape()` function for that purpose.

```{r}
speeches_para <- corpus_reshape(corpus_us_debate_speaker, to = "paragraphs")
head(summary(speeches_para))
```

## Topic model exercise: Reshape to paragraphs?

The paragraph splitter does not always produce very good results.

```{r}
table(ntoken(speeches_para))
```

## Topic model exercise: Create subset and dfm

We'll only consider those paragraphs that contain at least 8 words, remove punctuation, numbers, stop words, and tokens with less than 2 characters.

```{r}
speeches_para <- corpus_subset(speeches_para, ntoken(speeches_para) > 7)

para_tokens <- tokens(speeches_para, 
               remove_punct = TRUE, 
               remove_numbers = TRUE) %>% 
  tokens_remove(stopwords()) %>% 
  tokens_select(min_nchar = 2)

para_dfm <- dfm(para_tokens)
```


## Topic model exercise: LDA Topic model

Quanteda does not have any built-in topic models but we can load the required functions from the `topicmodels`, the `seedlda`, the `stm`, or similar packages. The packages each support different types of topic models and come with different functions for further analysis. We will run an LDA model with 10 topic categories using the `seededlda` package.

```{r}
library(seededlda)
para_lda <- textmodel_lda(para_dfm, k = 10)
```

## Topic model exercise: Investigate topic model output

Let's look at the most important term for each topic

```{r}
terms(para_lda, 10)
```

## Topic model exercise: Plot most important terms

We can extract the beta coefficients for each word from the model output into a data frame and tidy them up a bit to plot the key words for each topic.

```{r}
terms_df <- as_tibble(para_lda$phi) %>% 
  mutate(topic = 1:10) %>% 
  gather(term, beta, -topic) %>% 
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta)
```

## Topic model exercise: Plot most important terms

Then we can plot them using our familiar ggplot syntax.

```{r, eval = FALSE}
terms_df %>%
    mutate(term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free") +
    scale_y_reordered()
```

## Topic model exercise: Plot most important terms

```{r, echo = FALSE}
terms_df %>%
    mutate(term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free") +
    scale_y_reordered()
```

## Topic model exercise: Assign topics to documents

We can use the `topics()` function from the `seedlda` package to obtain the most likely topic for each document and assign them as a new document-level variable.

```{r}
para_dfm$topic <- topics(para_lda)
```

## Topic model exercise: Create topic labels

We can use the most important terms to create a label for each topic that helps us to differentiate between them.

```{r}
top_terms <- terms(para_lda, 4) 
topic_names <- apply(top_terms, 2, paste, collapse="_")
```

## Topic model exercise: Create topic labels

```{r, echo = FALSE}
topic_names
```

## Topic model exercise: Plot topic distribution

Similar to how we plotted the most important words per topic we can also extract the gamma coefficients of the model to plot the prevalence of topics across all documents. 

```{r}
topic_names_df <- dplyr::bind_rows(topic_names) %>% 
  gather(topic, names)

topics_df <- as_tibble(para_lda$theta) %>% 
  mutate(document = rownames(.)) %>% 
  gather(topic, gamma, -document) %>% 
  group_by(topic) %>%
  summarise(gamma = mean(gamma)) %>%
  arrange(desc(gamma)) %>% 
  left_join(topic_names_df, by = "topic")
```

## Topic model exercise: Plot most important terms

Then we can plot them using our familiar ggplot syntax.

```{r, eval = FALSE}
topics_df %>%
  ggplot(aes(reorder(topic, gamma), gamma, label = names, fill = topic)) +
  geom_col(show.legend = FALSE) +
  geom_text(hjust = 0, nudge_y = 0.0005, size = 3) +
  coord_flip() +
  scale_y_continuous(expand = c(0,0),
                     limits = c(0, 0.2),
                     labels = scales::percent_format()) +
  labs(x = NULL, y = expression(gamma),
       title = "Topic Prevalence",
       subtitle = "With the top words that contribute to each topic")
```

## Topic model exercise: Plot most important terms

```{r, echo = FALSE}
topics_df %>%
  ggplot(aes(reorder(topic, gamma), gamma, label = names, fill = topic)) +
  geom_col(show.legend = FALSE) +
  geom_text(hjust = 0, nudge_y = 0.0005, size = 3) +
  coord_flip() +
  scale_y_continuous(expand = c(0,0),
                     limits = c(0, 0.2),
                     labels = scales::percent_format()) +
  labs(x = NULL, y = expression(gamma),
       title = "Topic Prevalence",
       subtitle = "With the top words that contribute to each topic")
```

# Assignment 2

## Assignment 2

So far, we have looked only at one variant of the topic model. For the 2nd assignment you will explore the structural topic model from the `stm` package along with various diagnostic functions. You find the instructions for the assignment on GitHub and Moodle. 

Due date: 30 November 2021
Submission form: RMarkdown document 


# Wrapping up

## Questions?

## Outlook for our next session

Next week we will look at the very powerful `spacyr` package.

## That's it for today

Thanks for your attention!
